model,memory,latency,accuracy,precision,recall,cross,verbosity,much_explanation
llama3-8b,1.0,4.769939428662308,2.0000000000000004,1.5367965367965373,1.0,1.9090909090909094,1.8670694864048336,4.132930513595166
mixtral-8x7b,4.78914910505083,1.0,1.0,1.0,3.00749063670412,1.0,1.9924471299093653,4.007552870090635
gemma2-9b,5.0,5.0,2.6666666666666665,1.5367965367965373,1.0,5.0,5.0,1.0
llama-3.3-70b,4.993668672939959,4.404401635134176,5.0,5.0,5.0,4.818181818181818,1.0,5.0
